import os
from dotenv import load_dotenv
import boto3
import requests
import logging
import io
import pyarrow.parquet as pq
import pyarrow as pa
from datetime import datetime
import pandas as pd

load_dotenv()


def staging_static_data(filename, source='s3'):
    SOURCE_BUCKET = os.getenv("SOURCE_BUCKET")
    DEST_BUCKET = os.getenv("DEST_BUCKET")
    URL='https://docs.google.com/spreadsheets/d/1bQTdflSmSTkbMm2HLmLkKDcJlwv1S2CnbOxzbS6QcnQ/export?format=csv&gid=0#gid=0'
    s3 = boto3.client(
        "s3",
        aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
        aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
        region_name=os.getenv("AWS_DEFAULT_REGION")
    )
    if source=='s3':
        # Copy from source S3 bucket to destination S3 bucket
        try:
            key=f'customers/{filename}.csv'
            s3.copy_object(
                Bucket=DEST_BUCKET,
                CopySource={"Bucket": SOURCE_BUCKET, "Key": key},
                Key=f"raw/{filename}.csv"
            )        
            logging.info('Data loading sucessfully')
        except Exception as e:
            logging.error(f'Error while loading to S3, {e}')
        
    else:
        # Download from URL and upload to destination S3 bucket
        response = requests.get(URL)
        response.raise_for_status()     # Ensure download succeeded
        csv_bytes = response.content    
        try:
            s3.put_object(
                Bucket=DEST_BUCKET,
                Key=f"raw/{filename}.csv",
                Body= csv_bytes 
            )
            logging.info('Data loaded successfully')
        except Exception as e:
            logging.error(f'Error while loading to S3, {e}')

    


def write_parquet_to_s3(df, bucket, key,s3):

    table = pa.Table.from_pandas(df)

    out_buffer = io.BytesIO()
    pq.write_table(table, out_buffer)
    
    s3.put_object(
        Bucket=bucket,
        Key=key,
        Body=out_buffer.getvalue()
    )

    print(f"Wrote parquet → s3://{bucket}/{key}")


def ingest_call_center_logs():
    s3 = boto3.client("s3")

    source_bucket = os.getenv("SOURCE_BUCKET")
    prefix = "call logs/"
    RAW_BUCKET=os.getenv("DEST_BUCKET")

    s3 = boto3.client(
        "s3",
        aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
        aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
        region_name=os.getenv("AWS_DEFAULT_REGION")
    )

    response = s3.list_objects_v2(
        Bucket=source_bucket,
        Prefix=prefix
    )

    for obj in response.get("Contents", []):
        key = obj["Key"]

        if not key.endswith(".csv"):
            continue

        filename = key.split("/")[-1]
        date_part = filename.replace("call_logs_day_", "").replace(".csv", "")

        # Target output partition
        #target_key = f"raw/call_center_logs/ingest_date={date_part}/logs.parquet"
        target_key = f"raw/call_center_logs/call_logs_day_{date_part}.parquet"

        # CHECK IF the output already exists
        try:
            s3.head_object(Bucket=RAW_BUCKET, Key=target_key)
            print(f"File: {filename} — has already been processed.")
            continue
        except:
            pass

        print(f"Processing new file → {filename}")

        # Read the CSV
        file_obj = s3.get_object(Bucket=source_bucket, Key=key)
        df = pd.read_csv(io.BytesIO(file_obj["Body"].read()))

        # Standardize
        df.columns = [c.lower().replace(" ", "_") for c in df.columns]
        df["ingestion_timestamp"] = datetime.utcnow()
        df["source_system"] = "call_center_logs"
        df["file_name"] = filename

        # Write to RAW zone (Parquet)
        write_parquet_to_s3(df, RAW_BUCKET, target_key,s3)

        print(f"Ingested {filename} → {target_key}")

def ingest_social_media_data():
    s3 = boto3.client("s3")

    source_bucket = os.getenv("SOURCE_BUCKET")
    prefix = "social_medias/"
    RAW_BUCKET= os.getenv("DEST_BUCKET")

    s3 = boto3.client(
        "s3",
        aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
        aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
        region_name=os.getenv("AWS_DEFAULT_REGION")
    )

    response = s3.list_objects_v2(
        Bucket=source_bucket,
        Prefix=prefix
    )

    for obj in response.get("Contents", []):
        key = obj["Key"]

        if not key.endswith(".json"):
            continue

        filename = key.split("/")[-1]
        date_part = filename.replace("media_complaint_day_", "").replace(".json", "")

        # Target output partition
        #target_key = f"raw/call_center_logs/ingest_date={date_part}/logs.parquet"
        target_key = f"raw/social_media/media_complaint_day_{date_part}.parquet"

        # CHECK IF the output already exists
        try:
            s3.head_object(Bucket=RAW_BUCKET, Key=target_key)
            print(f"File: {filename} — has already been processed.")
            continue
        except:
            pass

        print(f"Processing new file → {filename}")

        # Read the CSV
        file_obj = s3.get_object(Bucket=source_bucket, Key=key)
        df = pd.read_json(io.BytesIO(file_obj["Body"].read()))

        # Standardize
        df.columns = [c.lower().replace(" ", "_") for c in df.columns]
        df["ingestion_timestamp"] = datetime.utcnow()
        df["source_system"] = "media_complaints"
        df["file_name"] = filename

        # Write to RAW zone (Parquet)
        write_parquet_to_s3(df, RAW_BUCKET, target_key,s3)

        print(f"Ingested {filename} → {target_key}")

